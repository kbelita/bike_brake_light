{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our evaluation functions¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Upsample the minority class (brakes applied)\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def upsample(input_df,col=\"Brake\",majority_value=0):\n",
    "    '''\n",
    "    Creates a balanced data set from the dataframe provided to it by upsampling the\n",
    "    minority class, using col as the column_name of classes to be balanced\n",
    "    '''    \n",
    "    #Split by row based on the data class\n",
    "    df_majority = input_df[input_df[col]==majority_value]\n",
    "    df_minority = input_df[input_df[col]!=majority_value]\n",
    "    \n",
    "    df_minority_upsampled = resample(df_minority,\n",
    "                                  replace=True,\n",
    "                                  n_samples=df_majority.shape[0],\n",
    "                                  random_state=444)\n",
    "    \n",
    "    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "    #print(\"Before upsampling:\\n\",df.Brake.value_counts())\n",
    "    #print(\"After upsampling:\\n\",df_upsampled.Brake.value_counts())\n",
    "    #print(df_upsampled.describe())\n",
    "    \n",
    "    return df_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from yellowbrick.classifier.classification_report import ClassificationReport\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "\n",
    "def fit_and_evaluate(df_data,model,label):\n",
    "    \"\"\"\n",
    "    Performs K-fold cross validation to create our evaluation scores, and then retrains the model\n",
    "    on the entire data set. \n",
    "    \n",
    "    df_data: a dataframe of the data to be modeled, with 'y' as the last column\n",
    "    model: the sklearn model class that we want to create a new instance of\n",
    "    label: string printed above the output; not stored in any way\n",
    "    \"\"\"\n",
    "    \n",
    "    #setup output variables\n",
    "    scores={'precision':[],'recall':[],'accuracy':[], 'f1':[]}\n",
    "    cm_list=[]\n",
    "    cm=np.array([[0,0],[0,0]])\n",
    "    \n",
    "    #Perform cross validation training\n",
    "    for train, test in KFold(df_data.shape[0], n_folds=12,shuffle=True):\n",
    "        df_train, df_test = df_data.iloc[train], df_data.iloc[test]\n",
    "        \n",
    "        df_train_upsampled = upsample(df_train,col=\"Brake\",majority_value=0)\n",
    "        \n",
    "        X_train, y_train = df_train_upsampled.iloc[:,:-1], df_train_upsampled.iloc[:,-1]\n",
    "        X_test, y_test = df_test.iloc[:,:-1], df_test.iloc[:,-1]\n",
    "        \n",
    "        estimator = model()\n",
    "        estimator.fit(X_train, y_train)\n",
    "        \n",
    "        expected  = y_test\n",
    "        predicted = estimator.predict(X_test)\n",
    "        \n",
    "        # Append our scores to the tracker\n",
    "        scores['precision'].append(metrics.precision_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['recall'].append(metrics.recall_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['accuracy'].append(metrics.accuracy_score(expected, predicted))\n",
    "        scores['f1'].append(metrics.f1_score(expected, predicted, average=\"weighted\"))\n",
    "        \n",
    "        cm_list.append(confusion_matrix(expected,predicted))\n",
    "    \n",
    "    for c in cm_list:\n",
    "        cm = np.add(cm,cm_list[0])\n",
    "    \n",
    "    # Retrain the model on the whole data set\n",
    "    estimator = model()\n",
    "    df_train_upsampled = upsample(df_train,col=\"Brake\",majority_value=0)\n",
    "    estimator.fit(df_train_upsampled.iloc[:,:-1], df_train_upsampled.iloc[:,-1])\n",
    "    \n",
    "    print(label)\n",
    "    print('-----------------')\n",
    "    print(pd.DataFrame(scores).mean())\n",
    "    print(cm)\n",
    "    \n",
    "    return estimator,scores,cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_comparison_file(estimator,output_name):\n",
    "    \"\"\"\n",
    "    Takes a fitted model, runs it on our unseen data set, and outputs the resulting predictions to a file.\n",
    "    \"\"\"\n",
    "    test_data_path = 'data/2018-01-31.csv'\n",
    "    test_df = pd.read_csv(test_data_path, sep=',', header=0)\n",
    "    test_df = test_df.iloc[:,3:]\n",
    "    \n",
    "    test_X = test_df.iloc[:,:-1]\n",
    "    test_y_actual = test_df.iloc[:,-1]\n",
    "    \n",
    "    test_y_predicted = estimator.predict(test_X)\n",
    "    print(confusion_matrix(test_y_actual,test_y_predicted))\n",
    "    \n",
    "    #confusion matrix viz\n",
    "    visualizer = ConfusionMatrix(estimator, classes=[0,1])\n",
    "    visualizer.score(test_X, test_y_actual)\n",
    "    visualizer.poof()\n",
    "    \n",
    "    #classification report\n",
    "    visualizer = ClassificationReport(estimator, classes=['No Brake','Brake'])\n",
    "    visualizer.score(test_X, test_y_actual)  \n",
    "    g = visualizer.poof() \n",
    "    \n",
    "    test_comparison_df = pd.DataFrame({'actual':test_y_actual,'predicted':test_y_predicted})\n",
    "    output_path = 'outputs/' + output_name\n",
    "    test_comparison_df.to_csv(output_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the actual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data/2018-01-29.csv\"\n",
    "\n",
    "# Get our 3 target columns = accel in each direction plus boolean (1/0) for the class braking/not braking\n",
    "df = pd.read_csv(data_path, sep=\",\",header=0)\n",
    "df = df.iloc[:,3:] #don't use first two columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train various models and compare them to a separate dataset¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classification_models = [KNeighborsClassifier,RandomForestClassifier,SVC]\n",
    "model_labels = [\"KN_Classifier\",\"Random_Forest\",\"SVM\"]\n",
    "\n",
    "for (model,label) in zip(classification_models,model_labels):\n",
    "    data_path = \"data/2018-01-29.csv\"\n",
    "    df = pd.read_csv(data_path, sep=\",\",header=0)\n",
    "    df = df.iloc[:,3:] #don't use first two columns\n",
    "    fitted_model, scores, cm = fit_and_evaluate(df,model,label)\n",
    "    file_name = label + \"_comparison.csv\"\n",
    "    create_comparison_file(fitted_model,file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from yellowbrick.features import RadViz,Rank2D\n",
    "from yellowbrick.features.pca import PCADecomposition\n",
    "from yellowbrick.features.pcoords import ParallelCoordinates\n",
    "\n",
    "def feature_viz(df,features,labels,classes):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # Extract the numpy arrays from the data frame\n",
    "    X = df[features].as_matrix()\n",
    "    y = df[labels].as_matrix()\n",
    "    \n",
    "    \n",
    "    visualizers = [ParallelCoordinates(classes=classes, features=features,normalize='standard', sample=0.1,),\n",
    "                   RadViz(classes=classes, features=features),\n",
    "                   Rank2D(features=features, algorithm='pearson'),\n",
    "                   PCADecomposition(scale=True, center=False, col=y)]\n",
    "    \n",
    "    for visualizer in visualizers:\n",
    "        visualizer.fit(X, y)                # Fit the data to the visualizer\n",
    "        visualizer.transform(X)             # Transform the data\n",
    "        visualizer.poof()                   # Draw/show/poof the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Upsample the minority class (brakes applied)\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def upsample(input_df,col=\"Brake\",majority_value=0):\n",
    "    '''\n",
    "    Creates a balanced data set from the dataframe provided to it by upsampling the\n",
    "    minority class, using col as the column_name of classes to be balanced\n",
    "    '''    \n",
    "    #Split by row based on the data class\n",
    "    df_majority = input_df[input_df[col]==majority_value]\n",
    "    df_minority = input_df[input_df[col]!=majority_value]\n",
    "    \n",
    "    df_minority_upsampled = resample(df_minority,\n",
    "                                  replace=True,\n",
    "                                  n_samples=df_majority.shape[0],\n",
    "                                  random_state=444)\n",
    "    \n",
    "    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "    #print(\"Before upsampling:\\n\",df.Brake.value_counts())\n",
    "    #print(\"After upsampling:\\n\",df_upsampled.Brake.value_counts())\n",
    "    #print(df_upsampled.describe())\n",
    "    \n",
    "    return df_upsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_features(df_data):\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    new_feat = pd.DataFrame()\n",
    "    new_feat['mag'] = (np.sqrt(df_data['X']**2) + (df_data['Y']**2) +(df_data['Z']**2))\n",
    "    new_feat['X_diff'] = df_data['X'].diff()\n",
    "    new_feat['Y_diff'] = df_data['Y'].diff()\n",
    "    new_feat['Z_diff'] = df_data['Z'].diff()\n",
    "\n",
    "    for i in range(1,11):\n",
    "        for axis in ['X','Y','Z']:        \n",
    "            colname = axis + '_shift' + str(i)\n",
    "            new_feat[colname] = df_data[axis].shift(i)\n",
    "\n",
    "    for i in [2,3,4,5,6,7,8,9,10]:\n",
    "        for axis in ['X','Y','Z']:        \n",
    "            colname = axis + '_avg' + str(i)\n",
    "            new_feat[colname] = df_data[axis].rolling(i).mean().shift(1)   \n",
    "\n",
    "\n",
    "    axes = df_data.iloc[:,:-1]\n",
    "    poly = PolynomialFeatures(degree=3, interaction_only=True)\n",
    "    poly_val = pd.DataFrame(poly.fit_transform(axes))\n",
    "    poly_val.columns = '1','X1','X2','X3','X1X2','X1X3','X2X3','X1X2X3'\n",
    "    poly_val.drop(['1'],inplace=True,axis=1)\n",
    "    \n",
    "    new_feat = pd.merge(new_feat, poly_val, right_index=True, left_index=True)\n",
    "    df_data = pd.merge(new_feat,df, right_index=True, left_index=True)\n",
    "    df_data.dropna(inplace=True)\n",
    "    df_data.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "def select_features(df_data,feature_selection_model):\n",
    "    '''\n",
    "    Selects features based on selectfrommodel \n",
    "    '''    \n",
    "    df_data_upsampled = upsample(df_data,col=\"Brake\",majority_value=0)\n",
    "    model = feature_selection_model()\n",
    "    sfm = SelectFromModel(model)\n",
    "    sfm.fit(df_data_upsampled.iloc[:,:-1], df_data_upsampled.iloc[:,-1])\n",
    "    features = list(df_data_upsampled.iloc[:,:-1].iloc[:,sfm.get_support(indices=True)])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from yellowbrick.classifier.classification_report import ClassificationReport\n",
    "\n",
    "\n",
    "def fit_and_evaluate_new(df_data,feature_selection_model,model,label):\n",
    "    \"\"\"\n",
    "    Performs K-fold cross validation to create our evaluation scores, and then retrains the model\n",
    "    on the entire data set. \n",
    "    \n",
    "    df_data: a dataframe of the data to be modeled, with 'y' as the last column\n",
    "    model: the sklearn model class that we want to create a new instance of\n",
    "    label: string printed above the output; not stored in any way\n",
    "    \"\"\"\n",
    "    \n",
    "    #setup output variables\n",
    "    scores={'precision':[],'recall':[],'accuracy':[], 'f1':[]}\n",
    "    cm_list=[]\n",
    "    cm=np.array([[0,0],[0,0]])\n",
    "    \n",
    "    #add more features\n",
    "    df_data = add_features(df_data)\n",
    "    new_columns = select_features(df_data,feature_selection_model)\n",
    "    \n",
    "    new_columns.append('Brake')\n",
    "    df_data = df_data[new_columns]\n",
    "\n",
    "    \n",
    "    #Perform cross validation training\n",
    "    for train, test in KFold(df_data.shape[0], n_folds=12,shuffle=True):\n",
    "        df_train, df_test = df_data.iloc[train], df_data.iloc[test]\n",
    "        \n",
    "        \n",
    "        df_train_upsampled = upsample(df_train,col=\"Brake\",majority_value=0)\n",
    "        \n",
    "        X_train, y_train = df_train_upsampled.iloc[:,:-1], df_train_upsampled.iloc[:,-1]\n",
    "        X_test, y_test = df_test.iloc[:,:-1], df_test.iloc[:,-1]\n",
    "        \n",
    "        estimator = model()\n",
    "        estimator.fit(X_train, y_train)\n",
    "        \n",
    "        expected  = y_test\n",
    "        predicted = estimator.predict(X_test)\n",
    "        \n",
    "        # Append our scores to the tracker\n",
    "        scores['precision'].append(metrics.precision_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['recall'].append(metrics.recall_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['accuracy'].append(metrics.accuracy_score(expected, predicted))\n",
    "        scores['f1'].append(metrics.f1_score(expected, predicted, average=\"weighted\"))\n",
    "        \n",
    "        cm_list.append(confusion_matrix(expected,predicted))\n",
    "        \n",
    "\n",
    "    \n",
    "    for c in cm_list:\n",
    "        cm = np.add(cm,cm_list[0])\n",
    "        \n",
    "    \n",
    "    # Retrain the model on the whole data set df_train_upsampled was here - changed\n",
    "    estimator = model()\n",
    "    df_data_upsampled = upsample(df_data,col=\"Brake\",majority_value=0)\n",
    "    estimator.fit(df_data_upsampled.iloc[:,:-1], df_data_upsampled.iloc[:,-1])\n",
    "                                      \n",
    "    print(label)\n",
    "    print('-----------------')\n",
    "    print(pd.DataFrame(scores).mean())\n",
    "    print(cm)\n",
    "    print(new_columns)\n",
    "\n",
    "    \n",
    "    return estimator,scores,cm,new_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_comparison_file_new(estimator,output_name,columns):\n",
    "    \"\"\"\n",
    "    Takes a fitted model, runs it on our unseen data set, and outputs the resulting predictions to a file.\n",
    "    \"\"\"\n",
    "    test_data_path = 'data/2018-01-31.csv'\n",
    "    test_df = pd.read_csv(test_data_path, sep=',', header=0)\n",
    "    test_df = test_df.iloc[:,3:]\n",
    "    \n",
    "    test_df = add_features(test_df)\n",
    "    test_df = test_df[columns]\n",
    "    test_X = test_df.iloc[:,:-1]\n",
    "    test_y_actual = test_df.iloc[:,-1]\n",
    "    \n",
    "    test_y_predicted = estimator.predict(test_X)\n",
    "    print(confusion_matrix(test_y_actual,test_y_predicted))\n",
    "    \n",
    "    #confusion matrix viz\n",
    "    visualizer = ConfusionMatrix(estimator, classes=[0,1])\n",
    "    visualizer.score(test_X, test_y_actual)\n",
    "    visualizer.poof()\n",
    "    \n",
    "    #classification report\n",
    "    visualizer = ClassificationReport(estimator, classes=['No Brake','Brake'])\n",
    "    visualizer.score(test_X, test_y_actual)  \n",
    "    g = visualizer.poof() \n",
    "\n",
    "    test_comparison_df = pd.DataFrame({'actual':test_y_actual,'predicted':test_y_predicted})\n",
    "    output_path = 'outputs/' + output_name\n",
    "    test_comparison_df.to_csv(output_path);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the actual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data/2018-01-29.csv\"\n",
    "\n",
    "# Get our 3 target columns = accel in each direction plus boolean (1/0) for the class braking/not braking\n",
    "df = pd.read_csv(data_path, sep=\",\",header=0)\n",
    "df = df.iloc[:,3:] #don't use first two columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"X\",\"Y\",\"Z\"]\n",
    "classes = [\"no brake\",\"brake\"]\n",
    "labels = \"Brake\"\n",
    "feature_viz(df,features,labels,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data/2018-01-29.csv\"\n",
    "\n",
    "# Get our 3 target columns = accel in each direction plus boolean (1/0) for the class braking/not braking\n",
    "df = pd.read_csv(data_path, sep=\",\",header=0)\n",
    "df = df.iloc[:,3:] #don't use first two columns\n",
    "df.head()\n",
    "\n",
    "df = add_features(df)\n",
    "df.columns.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[[ 'X_diff', 'Y_diff', 'Z_diff','X','Y','Z','Brake']]\n",
    "features = [ 'X_diff', 'Y_diff', 'Z_diff','X','Y','Z']\n",
    "classes = [\"no brake\",\"brake\"]\n",
    "labels = \"Brake\"\n",
    "feature_viz(df,features,labels,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train various models and compare them to a separate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "feature_selection_models = [Lasso,Ridge,ElasticNet]\n",
    "feature_selection_labels = [\"Lasso\",\"Ridge\",\"ElasticNet\"]\n",
    "classification_models = [KNeighborsClassifier,RandomForestClassifier,SVC]\n",
    "model_labels = [\"KN_Classifier\",\"Random_Forest\",\"SVM\"]\n",
    "\n",
    "for (feature_selection_model,feature_selection_label) in zip(feature_selection_models,feature_selection_labels):\n",
    "    print (feature_selection_label)\n",
    "    print('-----------------\\n\\n')\n",
    "    for (model,label) in zip(classification_models,model_labels):\n",
    "        data_path = \"data/2018-01-29.csv\"\n",
    "        df = pd.read_csv(data_path, sep=\",\",header=0)\n",
    "        df = df.iloc[:,3:] #don't use first two columns\n",
    "        fitted_model, scores, cm, new_columns = fit_and_evaluate_new(df,feature_selection_model,model,label)\n",
    "        file_name = feature_selection_label + label + \"comparison.csv\"\n",
    "        create_comparison_file_new(fitted_model,file_name,new_columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python envpy3",
   "language": "python",
   "name": "envpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
